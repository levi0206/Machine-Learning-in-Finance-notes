{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Approximation Theorem\n",
    "## Outline\n",
    "- Introduction/universal approximation theorem by George Cybenko\n",
    "- Universal approximation theorem by the writer\n",
    "- Deep neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction/universal approximation theorem by George Cybenko\n",
    "The statement of the theorem is: for any continuous function $f$ on a compact set $K$, there exists a neural network with one hidden layer such that it converges to $f$ uniformly on $K$. I will present the proof given by George Cybenko [Approximation by Superpositions of a Sigmoidal Function](https://link.springer.com/article/10.1007/BF02551274) with some explanations.\n",
    "\n",
    "The paper demostrates that finite linear combinations of compositions of a fixed,  univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube, where the finite linear combination we want to discuss is of the form $$\\sum_{j=1}^N \\alpha_j\\sigma(y_j^{T}x+\\theta_j)$$ with $x,y_j\\in\\mathbb{R}^n$ and $\\alpha_j,\\theta_j$ fixed. Our major concern is with so-called sigmoidal $\\sigma$'s \n",
    "$$\n",
    "\\sigma(x) =\n",
    "\\begin{cases}\n",
    " 1 & x\\to\\infty \\\\\n",
    " 0 & x\\to-\\infty\n",
    "\\end{cases}\n",
    "$$\n",
    "In Hilbert's 13th problem, Kolmogorov showed that all continuous functions of $n$ variables have an exact representation in terms of finite superpositions and compositions of a small number of functions of one variable. \n",
    "> [Arnold-Kolmogorov representation theorem](https://en.wikipedia.org/wiki/Kolmogorov–Arnold_representation_theorem) (around 1960): functions on the unit cube can be represented by sums and uni-variate functions, i.e. for evey non-linear function of $d$ real variables one can finitely many functions of one real variable such that $$F(x_1,...,x_d)=\\sum_{i=1}^{2d}\\phi_i(\\sum_{j=1}^d \\psi_{ij}(x_j))$$\n",
    "\n",
    "However, the theorem only ensures the existence of a finite linear combination; the representation may involve different non-linear functions. Our interest is to approximate any continuous function with finite linear combination of the same univariate fucntions, sigmoidal functions. Other works such as [1](https://archive.nyu.edu/jspui/bitstream/2451/14329/1/IS-92-13.pdf) and [2](https://pinkus.net.technion.ac.il/files/2021/02/acta.pdf) show that sigmoidal functions are not necessary, but our focus now is Cybenko's proof."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure $\\mu$ is said to be regular if and only if the following conditions are satisfied\n",
    "1. $\\mu(K)<\\infty$ for all compact sets $K$\n",
    "2. $\\mu(E)=\\inf\\{\\mu(U) \\mid E\\subseteq U, U \\text{ is open}\\}$\n",
    "3. $\\mu(E)=\\sup\\{\\mu(K) \\mid K\\subseteq E, K \\text{ is compact}\\}$\n",
    "\n",
    "Intuitively, a measure is a mathematical tool that measures the \"size\" of a set. As their names suggest, regular measures has certain structural regularity. It behaves in the way that, in some sense, one would intuitively expect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will equip $C(I_n)$ with the sup norm. The sup norm of a function $f\\colon A\\to B\\subset\\mathbb{R}$ is defined as \n",
    "$$\n",
    "\\|f\\| = \\sup_{x\\in A} |f(x)|.\n",
    "$$\n",
    "A Borel measure on a topological space is a measure that is defined on all open sets. Write the space of signed regular Borel measure as $\\mathcal{M}(I_n)$.\n",
    "\n",
    "**Definition:** We say that $\\sigma$ is discriminatory if for a measure $\\mu\\in\\mathcal{M}(I_n)$\n",
    "$$\n",
    "\\int_{I_n} \\sigma(y^T x+\\theta)d\\mu(x)=0\n",
    "$$\n",
    "for all $y\\in\\mathbb{R}^n$ and $\\theta\\in\\mathbb{R}$ implies $\\mu=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to show is the following theorem:\n",
    "\n",
    "**Theorem:** Let $\\sigma$ be any continuous discriminatory function. Then finite sums of the form \n",
    "$$\n",
    "G(x)=\\sum_{j=1}^N \\alpha_j\\sigma(y_j^Tx+\\theta_j)\n",
    "$$\n",
    "is dense in $C(I_n)$. In other words, for any $f\\in C(I_n)$ and $\\varepsilon>0$, there exists a finite sum $G(x)$ such that\n",
    "$$\n",
    "|G(x)-f(x)|<\\varepsilon\n",
    "$$\n",
    "for all $x\\in C(I_n)$ (note that this is our claim at the beginning). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce the most important two theorems in the proof. \n",
    "\n",
    "**[Hahn-Banach theorem](https://en.wikipedia.org/wiki/Hahn–Banach_theorem):** Let X be a real vector space, $p$ a sublinear functional on $X$, $M$ a subspace of $X$, and $f$ a linear functional on $M$ such that $f(x)≤p(x)$ for all $x\\in M$. Then there exists a linear functional $F$ on $X$ such that $F(x)≤p(x)$ for all $x\\in X$ and $F\\vert_M=f$.\n",
    "\n",
    "**[Riesz Representation Theorem](https://en.wikipedia.org/wiki/Riesz_representation_theorem#Riesz_representation_theorem):** Let $C_c(I_n)$ denote the functions of compact support defined on $X$ and $\\chi_K$ is the indicator function of the compact set $K$. If $I$ is a positive linear functional on $C_c(I_n)$, then there is a unique Radon measure $\\mu$ on $X$ such that $I(f)=\\int fd\\mu$ for all $f\\in C_c(I_n)$. Moreoever, $\\mu$ satisfies \n",
    "$$\n",
    "\\mu(U)=\\sup\\{I(f)\\mid f\\in C_c(I_n), 0\\leq f\\leq 1,\\text{supp}(f)\\subset U\\} \\text{ for all open }U\\subset X\n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\mu(K)=\\inf\\{I(f)\\mid f\\in C_c(I_n), 0\\leq f\\leq 1,f\\geq \\chi_K\\} \\text{ for all open }K\\subset X.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:** Let $S$ be the set of all functions of the form $G(x)$. Clearly, it is a vector space and is a subspace of $C(I_n)$. We claim that the closure of $S$ is exactly $C(I_n)$.\n",
    "\n",
    "Suppose that the closure of $S$, say $\\bar{S}$, is not all of $C(I_n)$. By the Hahn-Banach Theorem, there exists a bounded linear functional $L$ on $C(I_n)$ such that $L(S)=L(\\bar{S})=0$ but $L\\neq 0$ (plug $f(x)=0$). By the Rieze Representation theorem, we can write $L$ in the following form \n",
    "$$\n",
    "L(f)=\\int_{I_n} f(x)d\\mu(x)\n",
    "$$\n",
    "for some $\\mu\\in I_n$ and all $f\\in C(I_n)$. Since\n",
    "$$\n",
    "\\sigma(y^T x+\\theta)\n",
    "$$\n",
    "is in $\\bar{S}$ for all $y\\in\\mathbb{R}^n$ and $\\theta\\in\\mathbb{R}$, we must have \n",
    "$$\n",
    "\\int_{I_n} \\sigma(y^T x+\\theta)d\\mu(x)=0.\n",
    "$$\n",
    "However, we assumed $\\sigma$ is discriminatory, so this condition implies $\\mu=0$, a contradiction. Thus, the space $S$ is dense in $C(I_n)$.$\\blacksquare$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proved that the sums of the form\n",
    "$$\n",
    "G(x)=\\sum_{j=1}^N \\alpha_j\\sigma(y_j^Tx+\\theta_j)\n",
    "$$\n",
    "is dense in $C(I_n)$ provided that $\\sigma$ is continuous and discriminatory. Now we show that any continuous sigmoidal function\n",
    "$$\n",
    "\\sigma(x) =\n",
    "\\begin{cases}\n",
    " 1 & x\\to\\infty \\\\\n",
    " 0 & x\\to-\\infty\n",
    "\\end{cases}\n",
    "$$\n",
    "is discriminatory. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma:** Any bounded, measurable siomoidal function, $\\sigma$, is discriminatory. In particular, any continuous sigmoidal function is discriminatory.\n",
    "\n",
    "**Proof:** For any $x,y,\\theta,\\varphi$, we have\n",
    "$$\n",
    "\\sigma(\\lambda(y^T x+\\theta)+\\varphi) =\n",
    "\\begin{cases}\n",
    " 1 & \\text{for } y^T x+\\theta>0 \\text{ and } \\lambda\\to\\infty \\\\\n",
    " 0 & \\text{for } y^T x+\\theta<0 \\text{ and } \\lambda\\to\\infty \\\\\n",
    " \\sigma(\\varphi) & \\text{for } y^T x+\\theta=0 \\text{ and all } \\lambda.\n",
    "\\end{cases}\n",
    "$$\n",
    "Thus $\\sigma_{\\lambda}(x)=\\sigma(\\lambda(y^T x+\\theta)+\\varphi)$ converges pointwisely and boundedly to \n",
    "$$\n",
    "\\gamma(x) =\n",
    "\\begin{cases}\n",
    " 1 & \\text{for } y^T x+\\theta>0 \\\\\n",
    " 0 & \\text{for } y^T x+\\theta<0 \\\\\n",
    " \\sigma(\\varphi) & \\text{for } y^T x+\\theta=0\n",
    "\\end{cases}\n",
    "$$\n",
    "as $\\lambda\\to\\infty$.\n",
    "\n",
    "Let $\\Pi_{y,\\theta}=\\{x\\mid y^Tx+\\theta=0\\}$ and $H_{y,\\theta}=\\{x\\mid y^T+\\theta>0\\}$. Since $|\\sigma_{\\lambda}(x)|\\leq \\max(1,\\sigma(\\varphi))$ for all $\\lambda$ and $x$, by dominated convergence theorem, we have\n",
    "$$\n",
    "\\lim_{\\lambda\\to\\infty} \\int_{I_n} \\sigma_{\\lambda}(x)d\\mu(x)=\\int_{I_n} \\lim_{\\lambda\\to\\infty}\\sigma_{\\lambda}(x)d\\mu(x)=\\int_{I_n} \\gamma(x)d\\mu(x)\n",
    "$$\n",
    "which implies\n",
    "$$\n",
    "0 = \\int_{I_n} \\sigma_{\\lambda}(x)d\\mu(x)= \\int_{I_n} \\gamma(x)d\\mu(x)= \\mu(H_{y,\\theta})+\\sigma(\\varphi)\\mu(\\Pi_{y,\\theta}).\n",
    "$$\n",
    "If $\\mu$ is a positive measure, then $\\mu(H_{y,\\theta})=\\mu(\\Pi_{y,\\theta})=0$. Then $\\mu=0$ and any continuous sigmoidal function is discriminatory.\n",
    "\n",
    "**Remark** It's easy to see that this holds for any sigmoidal $\\sigma$ with $\\sigma(t)\\to a$ as $t\\to\\infty$ and $\\sigma(t)\\to b$ as $t\\to-\\infty$.\n",
    "\n",
    "**Remark** The indicator function is a special case of $\\gamma$ by choosing $\\varphi$ such that $\\sigma(\\varphi)=1$.\n",
    "\n",
    "Fix $y$. For bounded measurable $h$, define the linear functional $F$ according to \n",
    "$$\n",
    "F(h)=\\int_{I_n} h(y^Tx)d\\mu(x)\n",
    "$$\n",
    "and note that $F$ is a bounded functional on $L^\\infty(\\mathbb{R})$ since $\\mu\\in\\mathcal{M}(I_n)$. Let $h$ be the indicator function on $[\\theta,\\infty)$, i.e.\n",
    "$$\n",
    "h(x)=\n",
    "\\begin{cases}\n",
    "1, & x\\geq\\theta \\\\\n",
    "0, & x<\\theta.\n",
    "\\end{cases}\n",
    "$$\n",
    "Hence,\n",
    "$$\n",
    "F(h)=\\int_{I_n} h(y^Tx)d\\mu(x)=\\mu(\\Pi_{y,-\\theta})+\\mu(H_{y,-\\theta})=0.\n",
    "$$\n",
    "Similarly, $F(h)=0$ if $h$ is the indicator function on $(\\theta,\\infty)$. Thus, $F(h)=0$ for the indicator function on any interval. Since simple functions are dense in $L^\\infty(\\mathbb{R})$, we conclude that $F=0$ which implies $\\mu=0$. Thus, the sigmoidal functions are discriminatory.$\\blacksquare$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation theorem by the writer\n",
    "The above proof is extended by replacing the inner product $y^Tx$ in sigmoidal functions by continuous (point separating) $f_i$ on $I_n$.\n",
    "\n",
    "Let $X$ be a compact space, i.e. a compact topological Hausdorff space, and let ${(f_i)}_i$ be a family of real valued continuous functions on $X$ which is point separating, closed under addition and contains the constants function.\n",
    "\n",
    "discriminatory:\n",
    "$$\n",
    "\\int_X \\phi(f_i(x) + \\beta_i) \\mu(dx) = 0 \n",
    "$$\n",
    "for all $i$ and for all $\\beta_i $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lemma:</b> Every bounded measurable sigmoidal function is discrimatory.\n",
    "\n",
    "Let $ \\phi $ a be a sigmoidal activation function and let $ \\mu $ be a signed Radon measure such that\n",
    "$$\n",
    "\\int_X \\phi(f_i(x) + \\beta_i) \\mu(dx) = 0 \n",
    "$$\n",
    "for all $ i $ and $\\beta_i $.\n",
    "\n",
    "By assumption $ \\phi(n f_i(x) + n \\theta_i + \\xi) $ approximates\n",
    "$$\n",
    "a 1_{\\{f_i + \\theta > 0 \\}} +  \\phi(\\xi) 1_{\\{f_i + \\theta = 0 \\}} + b 1_{\\{f_i + \\theta < 0 \\}} \n",
    "$$\n",
    "as the integers $ n \\to \\infty $ for all $ x \\in X$. By Lebesgue's dominated convergence theorem the signed measure $ \\mu $ satisfies therefore (notice that $\\xi$ is arbitrary!)\n",
    "$$\n",
    "\\int_X 1_{\\{f_i + \\theta_i > 0\\}} (x) \\mu(dx) = 0 \n",
    "$$\n",
    "for all $ i $ and for all $ \\theta_i $. Hence the signed Radon measure $ \\mu $ vanishes on all sets of the form $ f_i + \\theta_i > 0 $. \n",
    "\n",
    "**Remark** This also holds for $f_i + \\theta_i <,= 0$.\n",
    "\n",
    "By linear combinations and dominated convergence one obtains that\n",
    "$$\n",
    "\\int_X h(f_i(x))\\mu(dx) = 0\n",
    "$$\n",
    "for all continuous functions $h:\\mathbb{R} \\to \\mathbb{R}$, since a continuous function $ h $ can be approximated pointwise by step functions.\n",
    "\n",
    "**Remark** In real analysis, it's a common technique to approximate a continous function by a sequence of step functions.\n",
    "\n",
    "Take now $ h(x) = \\sin(x) $ or $ h(x) = \\cos(x) $, then we obtain that actually\n",
    "$$\n",
    "\\int_X \\exp( \\sqrt{-1} \\sum f_i(x) k_i ) \\mu(dx) = 0\n",
    "$$\n",
    "for all integers $ k_i \\in \\mathbb{Z} $.\n",
    "\n",
    "**Remark** The following arguments are beyond my knowledge, so I just walk through the rest of proof.\n",
    "\n",
    "The closure of the linear space generated by $ h(f_i) $ for all $ i $ and for all continuous functions $ h $ contains an algebra, e.g.\n",
    "$$\n",
    "\\int_X f_{i_1}(x) \\cdots f_{i_k}(x) \\mu(dx) = 0\n",
    "$$\n",
    "for all $i_1,\\ldots,i_k$. This, however, means that the measure $ \\mu $ vanishes, since the algebra generated by the point separating function family $ (f_i) $ is dense due to the Stone-Weierstrass theorem. Thus, the sigmoidal functions are discriminatory.$\\blacksquare$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is universal approximation theorem important?\n",
    "**Theorem** Compactly supported continuous functions are dense in $L^p$ space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- universal approximtion theorem: neural networks with one hidden layer is dense in the space of continous functions $C(X)$\n",
    "- Are neural networks with $N$ hidden layers dense in $C(X)$?\n",
    "- It's still an open question. So far, there's no generic theory of this problem. \n",
    "- However, in practice we usually use deep neural networks instead of shallow neural networks.\n",
    "- Check this [paper](https://arxiv.org/abs/1611.00740) to see why deep neural networks are better if you're interested in this question.\n",
    "> The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "The author didn't provide the proof, so we just walk through these arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks with one hidden layer are called *shallow*, with more than one *deep*.\n",
    "\n",
    "The writer gives a series of arguments why deep neural networks represent so well non-linear functions following the famous article [Provable approximations for deep neural networks](https://arxiv.org/pdf/1509.07385.pdf). The most significant theorem in this article is :\n",
    "\n",
    "<b>$L^2$ functions with sparse wavelet expansion on $d$ dimensional manifolds $\\Gamma \\subset \\mathbb{R}^m$ can be approximated by depth $3$ sparsely connected neural. </b>\n",
    "\n",
    "\n",
    "Again, this is beyond my knowledge, so I will explain the construction of the proof as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $r(x)=\\max(x,0)$. Define\n",
    "$$\n",
    "t(x) = r(x+3) - r(x+1) - r(x-1) + r(x-3),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi(x_1,\\ldots,x_d) = C_d r \\big (\\sum_{i=1}^d t(x_i) - 2(d-1) \\big)\n",
    "$$\n",
    "which is already neural network of depth $2$ with a constant chosen such that $ \\int \\phi(x) dx = 1 $.*\n",
    "\n",
    "$\\phi$ is used to define wavelets (frames), which serve to expand non-linear $f$. Let $S_{-1}=0$. Define\n",
    "$$\n",
    "S_k(x,b) := 2^k \\phi (2^{k/d}(x-b))\n",
    "$$\n",
    "for $ k \\in \\mathbb{N} $, \n",
    "$$\n",
    "D_k(x,b) := S_k(x,b) - S_{k-1}(x,b)\n",
    "$$\n",
    "as well as $ \\psi_k(x) := 2^{-k/2} D_k(x,b) $, the 'mother' wavelet.\n",
    "\n",
    "Then $ \\{ \\psi_k(.,b) \\} $ for $ k \\in \\mathbb{Z} $ and $ b \\in 2^{-k} \\mathbb{Z}^d $ is a frame in $ L^2(\\mathbb{R}^d) $*.\n",
    "\n",
    "<b>Let $ f $ be a compactly supported function on $ \\mathbb{R}^d $ with bounded gradient, then for every $ K \\in \\mathbb{N} $ there is a linear combination $ f_K $ of frame elements up to order $ K $ such\n",
    "$$\n",
    "\\big|f(x) - f_K(x) \\big| = O\\big(2^{-2K/d}\\big) \\, .\n",
    "$$\n",
    "</b>\n",
    "\n",
    "Whence one can approximate the function $ f $ by neural networks of depth $2$ build by rectifier units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
